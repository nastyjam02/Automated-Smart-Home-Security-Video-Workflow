import os
import shutil
from pathlib import Path
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms  # æä¾›é¢„è®­ç»ƒæ¨¡å‹ã€å›¾åƒå˜æ¢å·¥å…·
import torchvision.models as models
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# å›ºå®šå‚æ•°
VIDEO_ROOT = "/remote-home/cr/video_Chain/videos/test_pipeline"        # # å¾…å¤„ç†è§†é¢‘çš„æ ¹ç›®å½•
TEMP_DIR = "/remote-home/cr/video_Chain/temp_dir"                  # ä¸´æ—¶å¸§å­˜æ”¾ç›®å½•
OUT_DIR = "/remote-home/cr/video_Chain/out1_dir"                  # å»é‡åå¸§å­˜æ”¾ç›®å½•
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

#  æŠ½å–è§†é¢‘å¸§
def extract_frames(video_path, output_dir, interval_sec):
    os.makedirs(output_dir, exist_ok=True)
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        raise IOError(f"Cannot open video: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps * interval_sec)  # è®¡ç®—é—´éš”å¸§æ•°
    count = 0
    saved = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if count % frame_interval == 0:
            frame_name = f"frame_{saved:05d}.jpg"
            cv2.imwrite(str(Path(output_dir) / frame_name), frame)
            saved += 1
        count += 1

    cap.release()
    return saved


def load_feature_extractor(device):
    model = models.resnet50(pretrained=True)  # ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
    modules = list(model.children())[:-1]  # å»æ‰åˆ†ç±»å¤´
    feature_extractor = nn.Sequential(*modules).to(device)
    feature_extractor.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ eval() ä»¥å…³é—­ Dropout/BatchNorm çš„è®­ç»ƒè¡Œä¸ºã€‚
    return feature_extractor

#  å•å¼ å›¾åƒç‰¹å¾æŠ½å–
def extract_feature(image_path, model, device, transform):
    img = cv2.imread(str(image_path))
    if img is None:
        raise IOError(f"Cannot read image: {image_path}")
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # è½¬ä¸º PIL Image
    img_pil = transforms.ToPILImage()(img)  #  transformï¼ˆåé¢å®šä¹‰ï¼‰å®Œæˆç¼©æ”¾ã€å½’ä¸€åŒ–ç­‰
    tensor = transform(img_pil).unsqueeze(0).to(device)

    with torch.no_grad():
        feat = model(tensor)  # ï¼ˆResNet50 å»å¤´ï¼‰æ— æ¢¯åº¦åœ°æå–ç‰¹å¾å‘é‡[1,2048,1,1]
    feat = feat.cpu().numpy().reshape(-1)  # å±•å¹³ä¸ºä¸€ç»´é•¿åº¦ 2048
    feat = feat / np.linalg.norm(feat)  #  L2 å½’ä¸€åŒ–ï¼ˆä½¿ç‰¹å¾æ¨¡é•¿ä¸º 1ï¼‰,å½’ä¸€åŒ–åï¼Œä½™å¼¦ç›¸ä¼¼åº¦å°±æ˜¯ä¸¤å‘é‡çš„ç‚¹ç§¯
    return feat


def deduplicate_frames(frame_dir, output_dir, model, device, threshold):
    os.makedirs(output_dir, exist_ok=True)
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])  # æ¯å¸§ç¼©æ”¾åˆ° 224Ã—224ï¼Œå¹¶åš ImageNet æ ‡å‡†å½’ä¸€åŒ–,é¢„è®­ç»ƒçš„ ResNet50 åœ¨ ImageNet ä¸Šè®­ç»ƒï¼Œè¾“å…¥æ—¶ç”¨äº†ç‰¹å®šçš„å‡å€¼å’Œæ–¹å·®åšå½’ä¸€åŒ–

    saved_features = []
    for img_name in sorted(os.listdir(frame_dir)):
        img_path = Path(frame_dir) / img_name
        feat = extract_feature(img_path, model, device, transform)  # æŠ½å–å½“å‰å¸§çš„ç‰¹å¾ feat
        if not saved_features:
            saved_features.append(feat)  # ç¬¬ä¸€ä¸ªå¸§ï¼Œç›´æ¥ä¿å­˜
            shutil.copy(str(img_path), str(Path(output_dir) / img_name))
        else:
            sims = cosine_similarity([feat], saved_features)[0]  # å¦åˆ™è®¡ç®—å®ƒä¸å·²ä¿å­˜å¸§ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦åˆ—è¡¨ sims
            if np.max(sims) < threshold:
                saved_features.append(feat)  # åªè¦æ‰€æœ‰ç›¸ä¼¼åº¦éƒ½ä½äº thresholdï¼ˆé»˜è®¤ 0.95ï¼‰ï¼Œå°±è§†ä¸ºâ€œæ–°â€å¸§ï¼Œä¿å­˜å¹¶æŠŠç‰¹å¾åŠ å…¥ saved_features
                shutil.copy(str(img_path), str(Path(output_dir) / img_name))
    return len(saved_features)


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Frame sampling and deduplication")
    parser.add_argument("--interval", type=float, default=2.0, help="é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--threshold", type=float, default=0.95, help="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    args = parser.parse_args()

    print(f"å¼€å§‹æ‰¹é‡å¤„ç†ç›®å½• {VIDEO_ROOT} ä¸‹çš„æ‰€æœ‰è§†é¢‘ï¼Œæ¯ {args.interval}s é‡‡æ ·ä¸€æ¬¡ï¼Œé˜ˆå€¼ {args.threshold}")
    print("æ­£åœ¨åŠ è½½ ResNet50ï¼Œè¯·è€å¿ƒç­‰å¾…å“¦ğŸˆ...")
    model = load_feature_extractor(DEVICE)

    for video_path in Path(VIDEO_ROOT).rglob("*.mp4"):
        rel = video_path.relative_to(VIDEO_ROOT).with_suffix("")
        temp_dir = Path(TEMP_DIR)
        out_dir = Path(OUT_DIR) / rel

        # æ¸…ç©ºä¸´æ—¶ç›®å½•
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
        temp_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nå¤„ç†è§†é¢‘: {video_path}")
        n = extract_frames(video_path, temp_dir, args.interval)
        print(f" æå– {n} å¸§è‡³ {temp_dir}")

        m = deduplicate_frames(temp_dir, out_dir, model, DEVICE, args.threshold)
        print(f" å»é‡åä¿å­˜ {m} å¸§è‡³ {out_dir}")

        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir)

    print("æ‰€æœ‰è§†é¢‘å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
        main()

